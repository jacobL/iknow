{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Nov 29 13:24:59 2019\n",
    "\n",
    "@author: sh.tseng\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import copy\n",
    "import datetime\n",
    "from bert_serving.client import BertClient\n",
    "import pymysql\n",
    "from apyori import apriori\n",
    "\n",
    "def Load_Jiebadict(conn):\n",
    "    dbCursor  =  conn.cursor ()\n",
    "    SQL_Str='SELECT jiebaword FROM rma.rma_jiebadict'\n",
    "    dbCursor.execute(SQL_Str)\n",
    "    results = dbCursor.fetchall()\n",
    "    dict_List=[]\n",
    "    for db_row in results:\n",
    "        dict_List.append(db_row[0])\n",
    "    userdict=pd.DataFrame(dict_List)\n",
    "    userdict.to_csv('dict/userdict.txt', header=False,index=False,encoding='utf8')\n",
    "    #同義字\n",
    "    SQL_Str='SELECT WORD,SYNONYMOUS FROM rma.rma_jieba_synon'\n",
    "    dbCursor.execute(SQL_Str)\n",
    "    results = dbCursor.fetchall()\n",
    "    dict_Name=[]\n",
    "    dict_Value=[]\n",
    "    for db_row in results:\n",
    "        dict_Name.append(db_row[0])\n",
    "        dict_Value.append(db_row[1])\n",
    "    tongyici_dict=dict(zip(dict_Name,dict_Value))\n",
    "    with open(\"dict/tongyici_dict.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(tongyici_dict, fp)\n",
    "\n",
    "    SQL_Str='SELECT stopword FROM rma.rma_stopword'\n",
    "    dbCursor.execute(SQL_Str)\n",
    "    results = dbCursor.fetchall()\n",
    "    stop_words=[]\n",
    "    for db_row in results:\n",
    "        stop_words.append(db_row[0])\n",
    "    with open(\"dict/stop_words.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(stop_words, fp)\n",
    "        \n",
    "    return userdict,tongyici_dict,stop_words\n",
    "#多句斷詞 建BOW,斷詞完變list\n",
    "def jiebacut(datalist,stop_words,tongyici_dict):\n",
    "    datalist_S = []\n",
    "    for i, text in enumerate(datalist):\n",
    "        line = ''\n",
    "        for w in jieba.cut(text, cut_all=False):\n",
    "            if w.lower() not in stop_words:\n",
    "                if w.lower() in tongyici_dict:\n",
    "                    tongyici_w=tongyici_dict[w.lower()]\n",
    "                else :\n",
    "                    if not bool(re.match('[0-9]+', w.lower())):\n",
    "                        tongyici_w=w.lower()\n",
    "                    else :\n",
    "                        tongyici_w=''\n",
    "                    \n",
    "                line=line+' '+ tongyici_w\n",
    "           \n",
    "        datalist_S.append(line)\n",
    "    cut_word_List=[]\n",
    "    for item in datalist_S:\n",
    "        data=[]\n",
    "        data=item.split(' ')\n",
    "        data_re = sorted(set(data),key=data.index)\n",
    "        while '' in data_re:\n",
    "            data_re.remove('')\n",
    "        cut_word_List.append(data_re)\n",
    "    return cut_word_List\n",
    "#單句斷詞不保留數字\n",
    "def jiebacut_word(Q_Word,stop_words,tongyici_dict):\n",
    "    Word_list = ''\n",
    "    for w in jieba.cut(Q_Word, cut_all=False):\n",
    "        if w.lower() not in stop_words:\n",
    "            if w.lower() in tongyici_dict:\n",
    "                tongyici_w=tongyici_dict[w.lower()]\n",
    "            else :\n",
    "                if not bool(re.match('[0-9]+', w.lower())):\n",
    "                    tongyici_w=w.lower()\n",
    "                else :\n",
    "                    tongyici_w=''\n",
    "            Word_list=Word_list+' '+ tongyici_w\n",
    "    cut_word=[]\n",
    "    cut_word=Word_list.split(' ')\n",
    "    while '' in cut_word:\n",
    "        cut_word.remove('')\n",
    "    cut_word_re = sorted(set(cut_word),key=cut_word.index)\n",
    "    return cut_word_re\n",
    "#單句斷詞保留數字\n",
    "def jiebacut_word_num(Q_Word,stop_words,tongyici_dict):\n",
    "    Word_list = ''\n",
    "    for w in jieba.cut(Q_Word, cut_all=False):\n",
    "        if w.lower() not in stop_words:\n",
    "            if w.lower() in tongyici_dict:\n",
    "                tongyici_w=tongyici_dict[w.lower()]\n",
    "            else :\n",
    "                tongyici_w=w.lower()\n",
    "            Word_list=Word_list+' '+ tongyici_w\n",
    "    cut_word=[]\n",
    "    cut_word=Word_list.split(' ')\n",
    "    while '' in cut_word:\n",
    "        cut_word.remove('')\n",
    "    cut_word_re = sorted(set(cut_word),key=cut_word.index)\n",
    "    return cut_word_re\n",
    "#多句斷詞,斷詞完變字串\n",
    "def jiebacut_WF(datalist,stop_words,tongyici_dict):\n",
    "    datalist_S = []\n",
    "    for i, text in enumerate(datalist):\n",
    "        line = ''\n",
    "        for w in jieba.cut(text, cut_all=False):\n",
    "            if w.lower() not in stop_words:\n",
    "                if w.lower() in tongyici_dict:\n",
    "                    tongyici_w=tongyici_dict[w.lower()]\n",
    "                else :\n",
    "#                     if not bool(re.match('[0-9]+', w.lower())):\n",
    "                        tongyici_w=w.lower()\n",
    "#                     else :\n",
    "#                         tongyici_w=''\n",
    "                line=line+' '+ tongyici_w\n",
    "           \n",
    "        datalist_S.append(line)\n",
    "    cut_word_List=[]\n",
    "    for item in datalist_S:\n",
    "        data=[]\n",
    "        data=item.split(' ')\n",
    "        data_re = sorted(set(data),key=data.index)\n",
    "        while '' in data_re:\n",
    "            data_re.remove('')\n",
    "        datastr=' '.join(data_re)\n",
    "        cut_word_List.append(datastr)\n",
    "    return cut_word_List\n",
    "#單句斷詞,斷詞完變字串\n",
    "def jiebacut_WF_word(Q_Word,stop_words,tongyici_dict):\n",
    "    Word_list = ''\n",
    "    for w in jieba.cut(Q_Word, cut_all=False):\n",
    "        if w.lower() not in stop_words:\n",
    "            if w.lower() in tongyici_dict:\n",
    "                tongyici_w=tongyici_dict[w.lower()]\n",
    "            else :\n",
    "                tongyici_w=w.lower()\n",
    "#                 if not bool(re.match('[0-9]+', w.lower())):\n",
    "#                     tongyici_w=w.lower()\n",
    "#                 else :\n",
    "#                     tongyici_w=''\n",
    "            Word_list=Word_list+' '+ tongyici_w\n",
    "    cut_word=[]\n",
    "    cut_word=Word_list.split(' ')\n",
    "    while '' in cut_word:\n",
    "        cut_word.remove('')\n",
    "    cut_word_re = sorted(set(cut_word),key=cut_word.index)\n",
    "    datastr=' '.join(cut_word_re)\n",
    "    return datastr\n",
    "\n",
    "def Build_BOW(conn,bc,stop_words,tongyici_dict):\n",
    "    #標準問法題庫\n",
    "    query=\"SELECT * FROM rma.rma_standand_question Where Enabled='True'\"   \n",
    "    Stand_Q_df = pd.read_sql(query, conn)\n",
    "    #Stand_Q_df = pd.read_csv('DB/Stand_Q.txt',encoding='utf-8',delimiter=\"\\t\")\n",
    "    Stand_Q=Stand_Q_df['Question'] #問題\n",
    "    Stand_Q_AnsNO=Stand_Q_df['Answer_NO'].tolist() #對應答案編號\n",
    "    Stand_Q_Ans=Stand_Q_df['Answer'].tolist() #對應答案編號\n",
    "    Stand_Q_SOP=Stand_Q_df['SOP_chapter'].tolist() #對應答案編號\n",
    "    Stand_Q_SOPNO=Stand_Q_df['SOP_No'].tolist() #對應答案編號\n",
    "    Stand_Q_URL=Stand_Q_df['URL'].tolist() #對應答案編號\n",
    "    Stand_Q_lower=Stand_Q.str.lower() #問題轉小寫\n",
    "    Stand_Q_lower=Stand_Q_lower.tolist() #轉成陣列\n",
    "    Stand_Q_list=Stand_Q.tolist()\n",
    "    Stand_A_dict=dict(zip(Stand_Q_list,Stand_Q_AnsNO))\n",
    "    with open(\"BOW/Stand_A_dict.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(Stand_A_dict, fp)\n",
    "#     print(Stand_Q_Ans)\n",
    "#     print(Stand_A_dict)\n",
    "    \n",
    "    Stand_Q_dict={}\n",
    "    i=0\n",
    "    for AnsNO in Stand_Q_AnsNO :\n",
    "        Stand_Q_Ans[i]=str(Stand_Q_Ans[i]).replace('\\r\\n','<br>')\n",
    "        Stand_Q_SOP[i]=str(Stand_Q_SOP[i]).replace('\\r\\n','<br>')\n",
    "        Stand_Q_SOPNO[i]=str(Stand_Q_SOPNO[i]).replace('','')\n",
    "        Stand_Q_URL[i]=str(Stand_Q_URL[i]).replace('','')\n",
    "        Stand_Q_dict[AnsNO]={'Question':Stand_Q_list[i],'Answer':Stand_Q_Ans[i]\n",
    "                             ,'SOP':Stand_Q_SOP[i],'SOP_No':Stand_Q_SOPNO[i],'URL':Stand_Q_URL[i]}\n",
    "        i+=1\n",
    "    with open(\"BOW/Stand_Q_dict.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(Stand_Q_dict, fp)    \n",
    "    #print(Stand_Q_dict)\n",
    "    \n",
    "    #相似問題題庫\n",
    "    query='SELECT * FROM rma.rma_sim_question'  \n",
    "    Sim_Q_df = pd.read_sql(query, conn)\n",
    "    #Sim_Q_df = pd.read_csv('DB/Sim_Q.txt',encoding='utf-8',delimiter=\"\\t\")\n",
    "    Sim_Q=Sim_Q_df['Question']\n",
    "    Sim_Q_AnsNO=Sim_Q_df['Answer_NO'].tolist()\n",
    "    Sim_Q_lower=Sim_Q.str.lower()\n",
    "    Sim_Q_lower=Sim_Q_lower.tolist()\n",
    "    #關鍵字\n",
    "    Stand_Q_KW=Stand_Q_df['Keyword'].str.lower()\n",
    "    Stand_Q_KW=Stand_Q_KW.tolist()\n",
    "    i=0\n",
    "    kw_list=[] #每個問題的關鍵字\n",
    "    Ans_list=[] #每個關鍵字組合對應的答案編號\n",
    "    for qk in Stand_Q_KW:\n",
    "        qk_list=[]\n",
    "        qk_list=qk.split('；')\n",
    "        for w in qk_list:\n",
    "            w_list=[]\n",
    "            w_list=w.split('，')\n",
    "            w_list = [word.replace('\\r\\n', '') for word in w_list]\n",
    "            w_list = [word.strip() for word in w_list]\n",
    "            w_set=set(w_list)\n",
    "            kw_list.append(w_set)\n",
    "            Ans_list.append(Stand_Q_AnsNO[i])\n",
    "        i+=1\n",
    "    with open(\"BOW/kw_list.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(kw_list, fp)\n",
    "    with open(\"BOW/Ans_list.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(Ans_list, fp)\n",
    "    \n",
    "    #建BOW\n",
    "    All_Q=copy.copy(Stand_Q_lower)\n",
    "    All_Q.extend(Sim_Q_lower)\n",
    "      \n",
    "    All_Q_WF=jiebacut_WF(All_Q,stop_words,tongyici_dict)\n",
    "    vectorizer = CountVectorizer(token_pattern='\\w+')\n",
    "    X = vectorizer.fit_transform(All_Q_WF)\n",
    "    feature_name = vectorizer.get_feature_names()\n",
    "    with open(\"BOW/WordFreq_X.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(feature_name, fp)\n",
    "    All_Q_df = pd.DataFrame(X.toarray(),columns=feature_name)\n",
    "    #詞頻矩陣\n",
    "    All_Q_BOW=All_Q_df.values\n",
    "    with open(\"BOW/WordFreq_Array.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(All_Q_BOW, fp)\n",
    "    #詞頻dataframe    \n",
    "    with open(\"BOW/WordFreq_DF.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(All_Q_df, fp)\n",
    "    All_Q_AnsNO=copy.copy(Stand_Q_AnsNO)\n",
    "    All_Q_AnsNO.extend(Sim_Q_AnsNO)    \n",
    "    with open(\"BOW/All_Q_AnsNO.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(All_Q_AnsNO, fp)\n",
    "    #讀取BERT\n",
    "    All_Q_BERT=bc.encode(All_Q)    \n",
    "    with open(\"BOW/All_Q_BERT.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(All_Q_BERT, fp)\n",
    "def Build_Association(conn):\n",
    "    query = \"select * from rma.rma_chat_log_new where answerNo is not null order by id desc\"\n",
    "    User_log_df = pd.read_sql(query, conn)\n",
    "    Q_sessionsId=User_log_df['session_id'].tolist()\n",
    "    Q_answerNo=User_log_df['answerNo'].tolist() \n",
    "    Q_dict={}\n",
    "    i=0\n",
    "    for qid in Q_sessionsId:\n",
    "        if qid in Q_dict:\n",
    "            Q_list=Q_dict[qid]\n",
    "            Q_list.append(Q_answerNo[i])\n",
    "            Q_dict[qid]=Q_list\n",
    "        else :\n",
    "            Q_dict[qid]=[Q_answerNo[i]]\n",
    "        i+=1\n",
    "    Q_basket=list(Q_dict.values())\n",
    "    association_rules = apriori(Q_basket, min_lift=0.5, max_length=2)\n",
    "    association_results = list(association_rules)\n",
    "    #print(association_results)\n",
    "    suggest_rules=[]\n",
    "    for r in association_results:\n",
    "        pair = r[0]\n",
    "        #print(pair)\n",
    "        rule = [x for x in pair]\n",
    "        if len(rule)>=2:\n",
    "            suggest_rules.append([rule[0],rule[1],str(r[1]),str(r[2][0][2]),str(r[2][0][3])])\n",
    "    #print(suggest_rules)\n",
    "    now_time=datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    cur  =  conn.cursor ()\n",
    "    for suggest in suggest_rules:\n",
    "        #print(suggest)\n",
    "        Q1=str(suggest[0])\n",
    "        Q2=str(suggest[1])\n",
    "        Support=suggest[2]\n",
    "        Confidence=suggest[3]\n",
    "        Lift=suggest[4]\n",
    "        log_sql=\"INSERT INTO rma.rma_association_rule (DateNO,Question_X,Question_Y,Support,Confidence,Lift) \\\n",
    "            VALUES ('\" + now_time + \"','\" + Q1 + \"','\"  + Q2 + \"','\" + Support + \"','\" + Confidence + \"','\" + Lift + \"')\"\n",
    "#         log_sql=\"INSERT INTO rma.rma_association_rule VALUES ('\" + now_time + \"','\" + Q1 + \"', \\\n",
    "#                 '\"  + Q2 + \"','\" + Support + \"','\" + Confidence + \"','\" + Lift + \"')\"\n",
    "        #print(log_sql)\n",
    "        cur.execute(log_sql)    \n",
    "        s=cur.execute(\"COMMIT\")\n",
    "    \n",
    "    return s\n",
    "def Build_Top_Question(conn):\n",
    "    DateNO=datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    start_time=(datetime.datetime.now() - datetime.timedelta(days=7)).strftime(\"%Y-%m-%d %H:%M\")\n",
    "    end_time=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    query = \"SELECT rma.rma_chat_log_new.answerNO,rma.rma_standand_question.category_main,\\\n",
    "    rma.rma_standand_question.category_sub,rma.rma_standand_question.Question \\\n",
    "    ,COUNT(rma.rma_chat_log_new.id) AS amount FROM rma.rma_chat_log_new inner join \\\n",
    "    rma.rma_standand_question on rma.rma_chat_log_new.answerNo=rma.rma_standand_question.Answer_NO \\\n",
    "    WHERE rma.rma_chat_log_new.checktime>='\" + start_time + \"' AND \\\n",
    "    rma.rma_chat_log_new.checktime<='\" + end_time + \"' AND \\\n",
    "    rma.rma_chat_log_new.answerNo IS NOT NULL AND rma.rma_chat_log_new.answerNo<>'' AND \\\n",
    "    rma.rma_chat_log_new.answerNo<>'AEXXXX' AND rma.rma_standand_question.system='RMA' \\\n",
    "    GROUP BY rma.rma_chat_log_new.answerNO ORDER BY amount DESC Limit 5\"\n",
    "    User_log_df = pd.read_sql(query, conn)\n",
    "    cur  =  conn.cursor ()\n",
    "    for i in range(0,len(User_log_df)):\n",
    "        insert_sql=\"INSERT INTO rma.rma_top_question (DateNO,System_NO,category_sub,Answer_NO,Question,amount) \\\n",
    "            VALUES ('\" + DateNO + \"','RMA','All','\"  + str(User_log_df.iloc[i,0]) + \"'\\\n",
    "            ,'\" + str(User_log_df.iloc[i,3]) + \"'\\\n",
    "            ,'\" + str(User_log_df.iloc[i,4]) +  \"')\"\n",
    "        cur.execute(insert_sql)\n",
    "        cur.execute(\"COMMIT\")\n",
    "    category_sub=['glossary','abnormal_handing','quality_system','specification_judge','process_production','quality_activity']\n",
    "    for c in category_sub:\n",
    "        query = \"SELECT rma.rma_chat_log_new.answerNO,rma.rma_standand_question.category_main,\\\n",
    "        rma.rma_standand_question.category_sub,rma.rma_standand_question.Question \\\n",
    "        ,COUNT(rma.rma_chat_log_new.id) AS amount FROM rma.rma_chat_log_new inner join \\\n",
    "        rma.rma_standand_question on rma.rma_chat_log_new.answerNo=rma.rma_standand_question.Answer_NO \\\n",
    "        WHERE rma.rma_chat_log_new.checktime>='\" + start_time + \"' AND \\\n",
    "        rma.rma_chat_log_new.checktime<='\" + end_time + \"' AND \\\n",
    "        rma.rma_standand_question.category_sub='\" + c + \"' AND \\\n",
    "        rma.rma_chat_log_new.answerNo IS NOT NULL AND rma.rma_chat_log_new.answerNo<>'' AND \\\n",
    "        rma.rma_chat_log_new.answerNo<>'AEXXXX' AND rma.rma_standand_question.system='RMA' \\\n",
    "        GROUP BY rma.rma_chat_log_new.answerNO ORDER BY amount DESC Limit 5\"\n",
    "        User_log_df = pd.read_sql(query, conn)\n",
    "        for i in range(0,len(User_log_df)):\n",
    "            insert_sql=\"INSERT INTO rma.rma_top_question (DateNO,System_NO,category_sub,Answer_NO,Question,amount) \\\n",
    "                VALUES ('\" + DateNO + \"','RMA','\" + str(User_log_df.iloc[i,2]) +\"','\"  + str(User_log_df.iloc[i,0]) + \"'\\\n",
    "                ,'\" + str(User_log_df.iloc[i,3]) + \"'\\\n",
    "                ,'\" + str(User_log_df.iloc[i,4]) +  \"')\"\n",
    "            cur.execute(insert_sql)\n",
    "            cur.execute(\"COMMIT\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        conn  =  pymysql.connect ( host = '10.55.52.98' ,port=33060 ,  user = 'root' ,  passwd = \"1234\"  )\n",
    "        userdict,tongyici_dict,stop_words=Load_Jiebadict(conn)\n",
    "        bc = BertClient(\"10.55.52.98\")\n",
    "        Build_BOW(conn,bc,stop_words,tongyici_dict)\n",
    "        Build_Association(conn)\n",
    "        Build_Top_Question(conn)\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        now_time=datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "        f = open('error/'+ now_time +'_BuildModel.txt','w')\n",
    "        f.write(str(e))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_Top_Question(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '...',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '^',\n",
       " '`',\n",
       " '§',\n",
       " '–',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '※',\n",
       " '、',\n",
       " '。',\n",
       " '〈',\n",
       " '〉',\n",
       " '《',\n",
       " '》',\n",
       " '「',\n",
       " '」',\n",
       " '『',\n",
       " '』',\n",
       " '【',\n",
       " '】',\n",
       " '〝',\n",
       " '〞',\n",
       " '︿',\n",
       " '﹏',\n",
       " '！',\n",
       " '＂',\n",
       " '＃',\n",
       " '＄',\n",
       " '％',\n",
       " '＆',\n",
       " '（',\n",
       " '）',\n",
       " '＊',\n",
       " '＋',\n",
       " '，',\n",
       " '－',\n",
       " '／',\n",
       " '：',\n",
       " '；',\n",
       " '＜',\n",
       " '＝',\n",
       " '＞',\n",
       " '？',\n",
       " '＠',\n",
       " '［',\n",
       " '＼',\n",
       " '］',\n",
       " '＿',\n",
       " '｛',\n",
       " '｜',\n",
       " '｝',\n",
       " '～']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
